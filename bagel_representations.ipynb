{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGEL Model Representation Extraction\n",
    "\n",
    "This notebook demonstrates how to load the BAGEL model and extract internal representations from different components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\n",
    "\n",
    "from data.data_utils import add_special_tokens, pil_img2rgb\n",
    "from data.transforms import ImageTransform\n",
    "from modeling.autoencoder import load_ae\n",
    "from modeling.bagel.qwen2_navit import NaiveCache\n",
    "from modeling.bagel import (\n",
    "    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM,\n",
    "    SiglipVisionConfig, SiglipVisionModel\n",
    ")\n",
    "from modeling.qwen2 import Qwen2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path - adjust if needed\n",
    "model_path = \"models/BAGEL-7B-MoT\"\n",
    "\n",
    "# Load configurations\n",
    "llm_config = Qwen2Config.from_json_file(os.path.join(model_path, \"llm_config.json\"))\n",
    "llm_config.qk_norm = True\n",
    "llm_config.tie_word_embeddings = False\n",
    "llm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n",
    "\n",
    "vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, \"vit_config.json\"))\n",
    "vit_config.rope = False\n",
    "vit_config.num_hidden_layers -= 1\n",
    "\n",
    "vae_model, vae_config = load_ae(local_path=os.path.join(model_path, \"ae.safetensors\"))\n",
    "\n",
    "config = BagelConfig(\n",
    "    visual_gen=True,\n",
    "    visual_und=True,\n",
    "    llm_config=llm_config, \n",
    "    vit_config=vit_config,\n",
    "    vae_config=vae_config,\n",
    "    vit_max_num_patch_per_side=70,\n",
    "    connector_act='gelu_pytorch_tanh',\n",
    "    latent_patch_size=2,\n",
    "    max_latent_size=64,\n",
    ")\n",
    "\n",
    "print(\"Configurations loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with empty weights\n",
    "with init_empty_weights():\n",
    "    language_model = Qwen2ForCausalLM(llm_config)\n",
    "    vit_model = SiglipVisionModel(vit_config)\n",
    "    model = Bagel(language_model, vit_model, config)\n",
    "    model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)\n",
    "\n",
    "# Image transforms\n",
    "vae_transform = ImageTransform(1024, 512, 16)\n",
    "vit_transform = ImageTransform(980, 224, 14)\n",
    "\n",
    "print(\"Model architecture created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device mapping for multi-GPU\n",
    "device_map = infer_auto_device_map(\n",
    "    model,\n",
    "    max_memory={i: \"80GiB\" for i in range(torch.cuda.device_count())},\n",
    "    no_split_module_classes=[\"Bagel\", \"Qwen2MoTDecoderLayer\"],\n",
    ")\n",
    "\n",
    "same_device_modules = [\n",
    "    'language_model.model.embed_tokens',\n",
    "    'time_embedder',\n",
    "    'latent_pos_embed',\n",
    "    'vae2llm',\n",
    "    'llm2vae',\n",
    "    'connector',\n",
    "    'vit_pos_embed'\n",
    "]\n",
    "\n",
    "if torch.cuda.device_count() == 1:\n",
    "    first_device = device_map.get(same_device_modules[0], \"cuda:0\")\n",
    "    for k in same_device_modules:\n",
    "        if k in device_map:\n",
    "            device_map[k] = first_device\n",
    "        else:\n",
    "            device_map[k] = \"cuda:0\"\n",
    "else:\n",
    "    first_device = device_map.get(same_device_modules[0])\n",
    "    for k in same_device_modules:\n",
    "        if k in device_map:\n",
    "            device_map[k] = first_device\n",
    "\n",
    "print(f\"Device map: {device_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights (using full precision - mode 1)\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint=os.path.join(model_path, \"ema.safetensors\"),\n",
    "    device_map=device_map,\n",
    "    offload_buffers=True,\n",
    "    offload_folder=\"offload\",\n",
    "    dtype=torch.bfloat16,\n",
    "    force_hooks=True,\n",
    ").eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_embeddings(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Extract text embeddings from the language model component.\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(next(model.parameters()).device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings from the language model\n",
    "        embeddings = model.language_model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        # Get hidden states from language model layers\n",
    "        hidden_states = model.language_model.model(input_ids, output_hidden_states=True)\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"hidden_states\": hidden_states.hidden_states,\n",
    "        \"last_hidden_state\": hidden_states.last_hidden_state\n",
    "    }\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract image features from both ViT and VAE encoders.\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # ViT preprocessing\n",
    "    vit_image = vit_transform(pil_img2rgb(image)).unsqueeze(0)\n",
    "    vit_image = vit_image.to(next(model.parameters()).device)\n",
    "    \n",
    "    # VAE preprocessing  \n",
    "    vae_image = vae_transform(pil_img2rgb(image)).unsqueeze(0)\n",
    "    vae_image = vae_image.to(next(model.parameters()).device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ViT features\n",
    "        vit_outputs = model.vit_model(vit_image, output_hidden_states=True)\n",
    "        vit_features = vit_outputs.last_hidden_state\n",
    "        vit_pooled = vit_outputs.pooler_output if hasattr(vit_outputs, 'pooler_output') else None\n",
    "        \n",
    "        # VAE features\n",
    "        vae_features = vae_model.encode(vae_image).latent_dist.sample()\n",
    "        \n",
    "    return {\n",
    "        \"vit_features\": vit_features,\n",
    "        \"vit_pooled\": vit_pooled,\n",
    "        \"vit_hidden_states\": vit_outputs.hidden_states,\n",
    "        \"vae_features\": vae_features,\n",
    "        \"original_image\": image\n",
    "    }\n",
    "\n",
    "def extract_multimodal_representations(text, image_path=None):\n",
    "    \"\"\"\n",
    "    Extract representations from the full multimodal pipeline.\n",
    "    \"\"\"\n",
    "    # Prepare inputs\n",
    "    inputs = {\"text\": text}\n",
    "    \n",
    "    if image_path:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs[\"image\"] = image\n",
    "        \n",
    "        # Process image through transforms\n",
    "        vit_image = vit_transform(pil_img2rgb(image)).unsqueeze(0)\n",
    "        vae_image = vae_transform(pil_img2rgb(image)).unsqueeze(0)\n",
    "        inputs[\"vit_image\"] = vit_image.to(next(model.parameters()).device)\n",
    "        inputs[\"vae_image\"] = vae_image.to(next(model.parameters()).device)\n",
    "    \n",
    "    # Tokenize text\n",
    "    text_inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = text_inputs[\"input_ids\"].to(next(model.parameters()).device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get text embeddings\n",
    "        text_embeds = model.language_model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        if image_path:\n",
    "            # Get image features\n",
    "            vit_outputs = model.vit_model(inputs[\"vit_image\"], output_hidden_states=True)\n",
    "            vae_features = vae_model.encode(inputs[\"vae_image\"]).latent_dist.sample()\n",
    "            \n",
    "            # Get cross-modal representations through connectors\n",
    "            vit_features_processed = model.connector(vit_outputs.last_hidden_state)\n",
    "            \n",
    "            return {\n",
    "                \"text_embeddings\": text_embeds,\n",
    "                \"vit_features\": vit_outputs.last_hidden_state,\n",
    "                \"vit_processed\": vit_features_processed,\n",
    "                \"vae_features\": vae_features,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"multimodal_ready\": True\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"text_embeddings\": text_embeds,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"multimodal_ready\": False\n",
    "            }\n",
    "\n",
    "print(\"Representation extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "sample_text = \"A beautiful sunset over the mountains with golden light.\"\n",
    "\n",
    "# Extract text representations\n",
    "text_repr = extract_text_embeddings(sample_text)\n",
    "\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Input IDs shape: {text_repr['input_ids'].shape}\")\n",
    "print(f\"Embeddings shape: {text_repr['embeddings'].shape}\")\n",
    "print(f\"Last hidden state shape: {text_repr['last_hidden_state'].shape}\")\n",
    "print(f\"Number of hidden layers: {len(text_repr['hidden_states'])}\")\n",
    "\n",
    "# Show embedding statistics\n",
    "embedding_mean = text_repr['embeddings'].mean().item()\n",
    "embedding_std = text_repr['embeddings'].std().item()\n",
    "print(f\"Embedding mean: {embedding_mean:.4f}, std: {embedding_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Image Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You'll need to provide an actual image path\n",
    "# Uncomment and modify the path below when you have an image to test\n",
    "\n",
    "# image_path = \"path/to/your/image.jpg\"\n",
    "# \n",
    "# # Extract image representations\n",
    "# image_repr = extract_image_features(image_path)\n",
    "# \n",
    "# print(f\"ViT features shape: {image_repr['vit_features'].shape}\")\n",
    "# print(f\"VAE features shape: {image_repr['vae_features'].shape}\")\n",
    "# print(f\"Number of ViT hidden layers: {len(image_repr['vit_hidden_states'])}\")\n",
    "# \n",
    "# # Show feature statistics\n",
    "# vit_mean = image_repr['vit_features'].mean().item()\n",
    "# vit_std = image_repr['vit_features'].std().item()\n",
    "# vae_mean = image_repr['vae_features'].mean().item()\n",
    "# vae_std = image_repr['vae_features'].std().item()\n",
    "# \n",
    "# print(f\"ViT features - mean: {vit_mean:.4f}, std: {vit_std:.4f}\")\n",
    "# print(f\"VAE features - mean: {vae_mean:.4f}, std: {vae_std:.4f}\")\n",
    "\n",
    "print(\"Uncomment the code above and provide an image path to test image feature extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Multimodal Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-only multimodal extraction\n",
    "multimodal_text = extract_multimodal_representations(\"Describe this beautiful landscape.\")\n",
    "\n",
    "print(\"Text-only multimodal extraction:\")\n",
    "print(f\"Text embeddings shape: {multimodal_text['text_embeddings'].shape}\")\n",
    "print(f\"Multimodal ready: {multimodal_text['multimodal_ready']}\")\n",
    "\n",
    "# Uncomment below for text + image multimodal extraction\n",
    "# image_path = \"path/to/your/image.jpg\"\n",
    "# multimodal_both = extract_multimodal_representations(\n",
    "#     \"What do you see in this image?\", \n",
    "#     image_path=image_path\n",
    "# )\n",
    "# \n",
    "# print(\"\\nText + Image multimodal extraction:\")\n",
    "# print(f\"Text embeddings shape: {multimodal_both['text_embeddings'].shape}\")\n",
    "# print(f\"ViT features shape: {multimodal_both['vit_features'].shape}\")\n",
    "# print(f\"ViT processed shape: {multimodal_both['vit_processed'].shape}\")\n",
    "# print(f\"VAE features shape: {multimodal_both['vae_features'].shape}\")\n",
    "# print(f\"Multimodal ready: {multimodal_both['multimodal_ready']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model components\n",
    "print(\"BAGEL Model Architecture:\")\n",
    "print(f\"Language model: {type(model.language_model).__name__}\")\n",
    "print(f\"Vision model: {type(model.vit_model).__name__}\")\n",
    "print(f\"VAE model: {type(vae_model).__name__}\")\n",
    "print(f\"\\nModel config:\")\n",
    "print(f\"- Visual generation: {config.visual_gen}\")\n",
    "print(f\"- Visual understanding: {config.visual_und}\")\n",
    "print(f\"- Max latent size: {config.max_latent_size}\")\n",
    "print(f\"- Latent patch size: {config.latent_patch_size}\")\n",
    "print(f\"- ViT max patches per side: {config.vit_max_num_patch_per_side}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_representations(representations, filename):\n",
    "    \"\"\"\n",
    "    Save representations to file for further analysis.\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy for saving\n",
    "    numpy_repr = {}\n",
    "    for key, value in representations.items():\n",
    "        if torch.is_tensor(value):\n",
    "            numpy_repr[key] = value.cpu().numpy()\n",
    "        elif isinstance(value, list) and torch.is_tensor(value[0]):\n",
    "            numpy_repr[key] = [v.cpu().numpy() for v in value]\n",
    "        else:\n",
    "            numpy_repr[key] = value\n",
    "    \n",
    "    np.savez(filename, **numpy_repr)\n",
    "    print(f\"Representations saved to {filename}\")\n",
    "\n",
    "# Example: Save text representations\n",
    "save_representations(text_repr, \"text_representations.npz\")\n",
    "\n",
    "# Load back example\n",
    "loaded_repr = np.load(\"text_representations.npz\", allow_pickle=True)\n",
    "print(f\"Loaded representation keys: {list(loaded_repr.keys())}\")\n",
    "print(f\"Loaded embeddings shape: {loaded_repr['embeddings'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides tools to:\n",
    "\n",
    "1. **Load the BAGEL model** following the official loading pattern\n",
    "2. **Extract text representations** from the language model component\n",
    "3. **Extract image features** from both ViT and VAE encoders\n",
    "4. **Extract multimodal representations** combining text and image processing\n",
    "5. **Analyze model architecture** and parameter counts\n",
    "6. **Save representations** for further analysis\n",
    "\n",
    "The extracted representations can be used for:\n",
    "- Understanding model internal states\n",
    "- Building interpretability tools\n",
    "- Creating safety mechanisms\n",
    "- Analyzing multimodal alignment\n",
    "- Fine-tuning experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}