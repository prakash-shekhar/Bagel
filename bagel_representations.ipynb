{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGEL Model Representation Extraction\n",
    "\n",
    "This notebook demonstrates how to load the BAGEL model and extract internal representations from different components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Bagel'...\n",
      "remote: Enumerating objects: 377, done.\u001b[K\n",
      "remote: Counting objects: 100% (243/243), done.\u001b[K\n",
      "remote: Compressing objects: 100% (152/152), done.\u001b[K\n",
      "remote: Total 377 (delta 136), reused 133 (delta 91), pack-reused 134 (from 2)\u001b[K\n",
      "Receiving objects: 100% (377/377), 2.24 MiB | 13.21 MiB/s, done.\n",
      "Resolving deltas: 100% (162/162), done.\n",
      "Collecting flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE\n",
      "  Downloading https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.1.post1/flash_attn-2.7.1.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl (183.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (2.6.0+cu124)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn==2.7.1.post1+cu12torch2.6cxx11abiFALSE) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed flash-attn-2.7.1.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "/content/Bagel\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ByteDance-Seed/Bagel.git\n",
    "!pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.1.post1/flash_attn-2.7.1.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
    "# Uninstall the incompatible flash-attention wheel\n",
    "# !pip uninstall -y flash_attn\n",
    "# Install flash-attention from source\n",
    "# !pip install --no-build-isolation flash-attn==2.6.1\n",
    "\n",
    "%cd Bagel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\n",
    "\n",
    "from data.data_utils import add_special_tokens, pil_img2rgb\n",
    "from data.transforms import ImageTransform\n",
    "from modeling.autoencoder import load_ae\n",
    "from modeling.bagel.qwen2_navit import NaiveCache\n",
    "from modeling.bagel import (\n",
    "    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM,\n",
    "    SiglipVisionConfig, SiglipVisionModel\n",
    ")\n",
    "from modeling.qwen2 import Qwen2Tokenizer\n",
    "\n",
    "# Assume the model is on Hugging Face Hub, adjust the repo ID if necessary\n",
    "!pip install huggingface_hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Model path - adjust if needed\n",
    "model_repo_id = \"ByteDance-Seed/Bagel-7B-MoT\" # Replace with the correct Hugging Face model ID\n",
    "model_path = \"models/BAGEL-7B-MoT\" # Local directory to save the downloaded files\n",
    "\n",
    "# Create the local directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# Download the necessary model files\n",
    "try:\n",
    "    hf_hub_download(repo_id=model_repo_id, filename=\"llm_config.json\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "    hf_hub_download(repo_id=model_repo_id, filename=\"vit_config.json\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "    hf_hub_download(repo_id=model_repo_id, filename=\"ae.safetensors\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "    hf_hub_download(repo_id=model_repo_id, filename=\"ema.safetensors\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "    # Also download the tokenizer files if they are separate\n",
    "    hf_hub_download(repo_id=model_repo_id, filename=\"tokenizer.json\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "    hf_hub_download(repo_id=model_repo_id, filename=\"tokenizer_config.json\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "    # hf_hub_download(repo_id=model_repo_id, filename=\"special_tokens_map.json\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "    # hf_hub_download(repo_id=model_repo_id, filename=\"tokenizer.model\", local_dir=model_path, local_dir_use_symlinks=False) # For SentencePiece\n",
    "    hf_hub_download(repo_id=model_repo_id, filename=\"vocab.json\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "    hf_hub_download(repo_id=model_repo_id, filename=\"merges.txt\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading model files: {e}\")\n",
    "    print(\"Please ensure the model ID is correct and the files exist on the Hugging Face Hub.\")\n",
    "    print(\"Alternatively, download the model files manually and place them in the './Bagel/models/BAGEL-7B-MoT' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path - adjust if needed\n",
    "model_path = \"models/BAGEL-7B-MoT\"\n",
    "\n",
    "# Load configurations\n",
    "llm_config = Qwen2Config.from_json_file(os.path.join(model_path, \"llm_config.json\"))\n",
    "llm_config.qk_norm = True\n",
    "llm_config.tie_word_embeddings = False\n",
    "llm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n",
    "\n",
    "vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, \"vit_config.json\"))\n",
    "vit_config.rope = False\n",
    "vit_config.num_hidden_layers -= 1\n",
    "\n",
    "vae_model, vae_config = load_ae(local_path=os.path.join(model_path, \"ae.safetensors\"))\n",
    "\n",
    "config = BagelConfig(\n",
    "    visual_gen=True,\n",
    "    visual_und=True,\n",
    "    llm_config=llm_config,\n",
    "    vit_config=vit_config,\n",
    "    vae_config=vae_config,\n",
    "    vit_max_num_patch_per_side=70,\n",
    "    connector_act='gelu_pytorch_tanh',\n",
    "    latent_patch_size=2,\n",
    "    max_latent_size=64,\n",
    ")\n",
    "\n",
    "print(\"Configurations loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with empty weights\n",
    "with init_empty_weights():\n",
    "    language_model = Qwen2ForCausalLM(llm_config)\n",
    "    vit_model = SiglipVisionModel(vit_config)\n",
    "    model = Bagel(language_model, vit_model, config)\n",
    "    model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)\n",
    "\n",
    "# Load tokenizer\n",
    "# Explicitly define paths to the required tokenizer files based on the repository structure\n",
    "vocab_file_path = os.path.join(model_path, \"vocab.json\")\n",
    "merges_file_path = os.path.join(model_path, \"merges.txt\")\n",
    "tokenizer_config_path = os.path.join(model_path, \"tokenizer_config.json\")\n",
    "special_tokens_path = os.path.join(model_path, \"special_tokens_map.json\")\n",
    "\n",
    "# Check if the necessary files exist\n",
    "if not os.path.exists(vocab_file_path):\n",
    "    raise FileNotFoundError(f\"Required tokenizer file not found: {vocab_file_path}\")\n",
    "if not os.path.exists(merges_file_path):\n",
    "    raise FileNotFoundError(f\"Required tokenizer file not found: {merges_file_path}\")\n",
    "# Optional checks for other files\n",
    "if not os.path.exists(tokenizer_config_path):\n",
    "    print(f\"Warning: Tokenizer config file not found at {tokenizer_config_path}. Tokenizer might load with default settings.\")\n",
    "if not os.path.exists(special_tokens_path):\n",
    "    print(f\"Warning: Special tokens map file not found at {special_tokens_path}. Special tokens might not be handled correctly.\")\n",
    "\n",
    "\n",
    "# Initialize the tokenizer by explicitly passing the paths\n",
    "# Based on standard Qwen2Tokenizer initialization, it expects vocab_file and merges_file\n",
    "try:\n",
    "    tokenizer = Qwen2Tokenizer(\n",
    "        vocab_file=vocab_file_path,\n",
    "        merges_file=merges_file_path,\n",
    "        # You might also pass other config files if needed and the constructor supports it\n",
    "        # e.g., tokenizer_config_file=tokenizer_config_path, special_tokens_map_file=special_tokens_path\n",
    "    )\n",
    "    print(\"Tokenizer initialized by explicitly providing file paths successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Explicit tokenizer initialization failed: {e}\")\n",
    "    print(\"Please verify the parameters accepted by the Qwen2Tokenizer constructor\")\n",
    "    print(\"in the Bagel repository's modeling/qwen2/tokenization_qwen2.py file.\")\n",
    "    # Re-raise the error as it indicates a deeper issue with file paths or constructor\n",
    "    raise e\n",
    "\n",
    "\n",
    "# Correct the incomplete line for adding special tokens\n",
    "# Assuming add_special_tokens is a function defined elsewhere (likely data.data_utils)\n",
    "tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)\n",
    "\n",
    "# Image transforms\n",
    "vae_transform = ImageTransform(1024, 512, 16)\n",
    "vit_transform = ImageTransform(980, 224, 14)\n",
    "\n",
    "print(\"Model architecture created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device mapping for multi-GPU\n",
    "device_map = infer_auto_device_map(\n",
    "    model,\n",
    "    max_memory={i: \"80GiB\" for i in range(torch.cuda.device_count())},\n",
    "    no_split_module_classes=[\"Bagel\", \"Qwen2MoTDecoderLayer\"],\n",
    ")\n",
    "\n",
    "same_device_modules = [\n",
    "    'language_model.model.embed_tokens',\n",
    "    'time_embedder',\n",
    "    'latent_pos_embed',\n",
    "    'vae2llm',\n",
    "    'llm2vae',\n",
    "    'connector',\n",
    "    'vit_pos_embed'\n",
    "]\n",
    "\n",
    "if torch.cuda.device_count() == 1:\n",
    "    first_device = device_map.get(same_device_modules[0], \"cuda:0\")\n",
    "    for k in same_device_modules:\n",
    "        if k in device_map:\n",
    "            device_map[k] = first_device\n",
    "        else:\n",
    "            device_map[k] = \"cuda:0\"\n",
    "else:\n",
    "    first_device = device_map.get(same_device_modules[0])\n",
    "    for k in same_device_modules:\n",
    "        if k in device_map:\n",
    "            device_map[k] = first_device\n",
    "\n",
    "print(f\"Device map: {device_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights (using full precision - mode 1)\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint=os.path.join(model_path, \"ema.safetensors\"),\n",
    "    device_map=device_map,\n",
    "    offload_buffers=True,\n",
    "    offload_folder=\"offload\",\n",
    "    dtype=torch.bfloat16,\n",
    "    force_hooks=True,\n",
    ").eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_embeddings(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Extract text embeddings from the language model component.\n",
    "    \n",
    "    Note: BAGEL uses custom packed input formats. We extract embeddings directly\n",
    "    and use forward hooks to capture intermediate representations.\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(next(model.parameters()).device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings directly from the embedding layer\n",
    "        embeddings = model.language_model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        # For BAGEL, we need to use the full model forward pass to get meaningful representations\n",
    "        # Create a simple text-only input for BAGEL\n",
    "        try:\n",
    "            # Use BAGEL's generate method to get representations during inference\n",
    "            # This is the proper way to access BAGEL's internal states\n",
    "            hidden_states_collected = []\n",
    "            \n",
    "            def collect_hidden_states(module, input, output):\n",
    "                if hasattr(output, 'last_hidden_state'):\n",
    "                    hidden_states_collected.append(output.last_hidden_state.detach())\n",
    "                elif isinstance(output, tuple) and len(output) > 0:\n",
    "                    hidden_states_collected.append(output[0].detach())\n",
    "            \n",
    "            # Register hook on the language model to capture output\n",
    "            hook = model.language_model.register_forward_hook(collect_hidden_states)\n",
    "            \n",
    "            # Create BAGEL-compatible input\n",
    "            # We'll use a minimal generation to trigger the forward pass\n",
    "            generation_outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=1,  # Minimal generation\n",
    "                do_sample=False,\n",
    "                return_dict_in_generate=True,\n",
    "                output_hidden_states=False,  # Don't request hidden states from generate\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Remove the hook\n",
    "            hook.remove()\n",
    "            \n",
    "            # Get the last hidden state from our collected states\n",
    "            last_hidden_state = hidden_states_collected[-1] if hidden_states_collected else None\n",
    "            \n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"embeddings\": embeddings,\n",
    "                \"hidden_states\": hidden_states_collected if hidden_states_collected else None,\n",
    "                \"last_hidden_state\": last_hidden_state,\n",
    "                \"generation_outputs\": generation_outputs\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Note: Full forward pass not available ({e}). Returning embeddings only.\")\n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"embeddings\": embeddings,\n",
    "                \"hidden_states\": None,\n",
    "                \"last_hidden_state\": None,\n",
    "                \"generation_outputs\": None\n",
    "            }\n",
    "\n",
    "def extract_text_embeddings_simple(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Simplified text embedding extraction - just embeddings and tokenization.\n",
    "    This always works regardless of BAGEL's complex forward methods.\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(next(model.parameters()).device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings directly from the embedding layer\n",
    "        embeddings = model.language_model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        # Get vocabulary size and embedding dimension\n",
    "        vocab_size = embeddings.shape[-1]\n",
    "        seq_len = embeddings.shape[1]\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"text\": text,\n",
    "        \"tokens\": tokenizer.convert_ids_to_tokens(input_ids[0]),\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"sequence_length\": seq_len,\n",
    "        \"embedding_dim\": vocab_size\n",
    "    }\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract image features from both ViT and VAE encoders.\n",
    "    Uses direct model access for the components that support it.\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # ViT preprocessing\n",
    "    vit_image = vit_transform(pil_img2rgb(image)).unsqueeze(0)\n",
    "    vit_image = vit_image.to(next(model.parameters()).device)\n",
    "    \n",
    "    # VAE preprocessing  \n",
    "    vae_image = vae_transform(pil_img2rgb(image)).unsqueeze(0)\n",
    "    vae_image = vae_image.to(next(model.parameters()).device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # VAE features - this should work as VAE is more standard\n",
    "        vae_features = vae_model.encode(vae_image).latent_dist.sample()\n",
    "        \n",
    "        # ViT features - try direct access first\n",
    "        try:\n",
    "            # BAGEL's ViT might also have custom input format\n",
    "            # Let's try the simpler approach first\n",
    "            vit_features = model.vit_model.vision_model.embeddings(vit_image)\n",
    "            vit_last_hidden = None\n",
    "            vit_hidden_states = None\n",
    "            \n",
    "            print(\"Note: Using direct ViT embedding access. Full ViT forward pass not implemented.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: ViT feature extraction failed: {e}\")\n",
    "            vit_features = None\n",
    "            vit_last_hidden = None\n",
    "            vit_hidden_states = None\n",
    "        \n",
    "    return {\n",
    "        \"vit_features\": vit_features,\n",
    "        \"vit_pooled\": None,\n",
    "        \"vit_hidden_states\": vit_hidden_states,\n",
    "        \"vae_features\": vae_features,\n",
    "        \"original_image\": image,\n",
    "        \"image_shape\": vit_image.shape,\n",
    "        \"vae_latent_shape\": vae_features.shape if vae_features is not None else None\n",
    "    }\n",
    "\n",
    "def extract_multimodal_representations_simple(text, image_path=None):\n",
    "    \"\"\"\n",
    "    Simplified multimodal representation extraction.\n",
    "    Gets embeddings and features that are directly accessible.\n",
    "    \"\"\"\n",
    "    # Get text representations\n",
    "    text_repr = extract_text_embeddings_simple(text)\n",
    "    \n",
    "    result = {\n",
    "        \"text_embeddings\": text_repr[\"embeddings\"],\n",
    "        \"input_ids\": text_repr[\"input_ids\"],\n",
    "        \"text\": text,\n",
    "        \"multimodal_ready\": False\n",
    "    }\n",
    "    \n",
    "    if image_path:\n",
    "        # Get image representations\n",
    "        image_repr = extract_image_features(image_path)\n",
    "        \n",
    "        result.update({\n",
    "            \"vit_features\": image_repr[\"vit_features\"],\n",
    "            \"vae_features\": image_repr[\"vae_features\"],\n",
    "            \"original_image\": image_repr[\"original_image\"],\n",
    "            \"multimodal_ready\": True\n",
    "        })\n",
    "        \n",
    "        # Try to get cross-modal features through BAGEL's connector\n",
    "        try:\n",
    "            if image_repr[\"vit_features\"] is not None:\n",
    "                # This might work if we can access the connector\n",
    "                vit_processed = model.connector(image_repr[\"vit_features\"])\n",
    "                result[\"vit_processed\"] = vit_processed\n",
    "        except Exception as e:\n",
    "            print(f\"Note: Cross-modal processing not available: {e}\")\n",
    "            result[\"vit_processed\"] = None\n",
    "    \n",
    "    return result\n",
    "\n",
    "def analyze_model_structure():\n",
    "    \"\"\"\n",
    "    Analyze BAGEL's structure to understand what components are accessible.\n",
    "    \"\"\"\n",
    "    print(\"BAGEL Model Structure Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Main model components\n",
    "    print(f\"Main model type: {type(model).__name__}\")\n",
    "    print(f\"Language model: {type(model.language_model).__name__}\")\n",
    "    print(f\"Vision model: {type(model.vit_model).__name__}\")\n",
    "    print(f\"VAE model: {type(vae_model).__name__}\")\n",
    "    \n",
    "    # Check for accessible components\n",
    "    print(\"\\nAccessible Components:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Text embeddings\n",
    "    try:\n",
    "        test_ids = torch.tensor([[1, 2, 3]]).to(next(model.parameters()).device)\n",
    "        embed_out = model.language_model.model.embed_tokens(test_ids)\n",
    "        print(f\"✓ Text embeddings: {embed_out.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Text embeddings: {e}\")\n",
    "    \n",
    "    # Connector\n",
    "    try:\n",
    "        test_vit = torch.randn(1, 256, 768).to(next(model.parameters()).device)\n",
    "        conn_out = model.connector(test_vit)\n",
    "        print(f\"✓ Connector: {test_vit.shape} -> {conn_out.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Connector: {e}\")\n",
    "    \n",
    "    # Time embedder\n",
    "    try:\n",
    "        if hasattr(model, 'time_embedder'):\n",
    "            print(f\"✓ Time embedder available\")\n",
    "        else:\n",
    "            print(f\"✗ Time embedder not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Time embedder: {e}\")\n",
    "    \n",
    "    print(\"\\nModel Configuration:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Visual generation: {config.visual_gen}\")\n",
    "    print(f\"Visual understanding: {config.visual_und}\")\n",
    "    print(f\"Max latent size: {config.max_latent_size}\")\n",
    "    print(f\"Connector activation: {config.connector_act}\")\n",
    "    \n",
    "    # Parameter count\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(\"BAGEL-compatible representation extraction functions defined!\")\n",
    "print(\"Use extract_text_embeddings_simple() for reliable text embeddings.\")\n",
    "print(\"Use analyze_model_structure() to see what components are accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "sample_text = \"A beautiful sunset over the mountains with golden light.\"\n",
    "\n",
    "# Extract text representations using the simple method (most reliable)\n",
    "text_repr = extract_text_embeddings_simple(sample_text)\n",
    "\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Input IDs shape: {text_repr['input_ids'].shape}\")\n",
    "print(f\"Embeddings shape: {text_repr['embeddings'].shape}\")\n",
    "print(f\"Tokens: {text_repr['tokens'][:10]}...\")  # Show first 10 tokens\n",
    "print(f\"Sequence length: {text_repr['sequence_length']}\")\n",
    "print(f\"Embedding dimension: {text_repr['embedding_dim']}\")\n",
    "\n",
    "# Show embedding statistics\n",
    "embedding_mean = text_repr['embeddings'].mean().item()\n",
    "embedding_std = text_repr['embeddings'].std().item()\n",
    "print(f\"Embedding mean: {embedding_mean:.4f}, std: {embedding_std:.4f}\")\n",
    "\n",
    "# Try the advanced extraction method (might work with generation)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Trying advanced extraction with generation...\")\n",
    "try:\n",
    "    text_repr_advanced = extract_text_embeddings(sample_text)\n",
    "    \n",
    "    if text_repr_advanced['hidden_states'] is not None:\n",
    "        print(f\"✓ Advanced extraction successful!\")\n",
    "        print(f\"Hidden states captured: {len(text_repr_advanced['hidden_states'])}\")\n",
    "        print(f\"Generation output shape: {text_repr_advanced['generation_outputs'].sequences.shape}\")\n",
    "    else:\n",
    "        print(\"ℹ️ Advanced extraction returned embeddings only\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Advanced extraction failed: {e}\")\n",
    "    print(\"Using simple extraction is recommended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Image Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You'll need to provide an actual image path\n",
    "# Uncomment and modify the path below when you have an image to test\n",
    "\n",
    "# image_path = \"path/to/your/image.jpg\"\n",
    "# \n",
    "# # Extract image representations\n",
    "# image_repr = extract_image_features(image_path)\n",
    "# \n",
    "# print(f\"ViT features shape: {image_repr['vit_features'].shape}\")\n",
    "# print(f\"VAE features shape: {image_repr['vae_features'].shape}\")\n",
    "# print(f\"Number of ViT hidden layers: {len(image_repr['vit_hidden_states'])}\")\n",
    "# \n",
    "# # Show feature statistics\n",
    "# vit_mean = image_repr['vit_features'].mean().item()\n",
    "# vit_std = image_repr['vit_features'].std().item()\n",
    "# vae_mean = image_repr['vae_features'].mean().item()\n",
    "# vae_std = image_repr['vae_features'].std().item()\n",
    "# \n",
    "# print(f\"ViT features - mean: {vit_mean:.4f}, std: {vit_std:.4f}\")\n",
    "# print(f\"VAE features - mean: {vae_mean:.4f}, std: {vae_std:.4f}\")\n",
    "\n",
    "print(\"Uncomment the code above and provide an image path to test image feature extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Multimodal Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-only multimodal extraction using the new simple method\n",
    "multimodal_text = extract_multimodal_representations_simple(\"Describe this beautiful landscape.\")\n",
    "\n",
    "print(\"Text-only multimodal extraction:\")\n",
    "print(f\"Text embeddings shape: {multimodal_text['text_embeddings'].shape}\")\n",
    "print(f\"Text: {multimodal_text['text']}\")\n",
    "print(f\"Multimodal ready: {multimodal_text['multimodal_ready']}\")\n",
    "\n",
    "# Uncomment below for text + image multimodal extraction\n",
    "# image_path = \"path/to/your/image.jpg\"\n",
    "# multimodal_both = extract_multimodal_representations_simple(\n",
    "#     \"What do you see in this image?\", \n",
    "#     image_path=image_path\n",
    "# )\n",
    "# \n",
    "# print(\"\\nText + Image multimodal extraction:\")\n",
    "# print(f\"Text embeddings shape: {multimodal_both['text_embeddings'].shape}\")\n",
    "# if multimodal_both['vit_features'] is not None:\n",
    "#     print(f\"ViT features shape: {multimodal_both['vit_features'].shape}\")\n",
    "# if multimodal_both['vit_processed'] is not None:\n",
    "#     print(f\"ViT processed shape: {multimodal_both['vit_processed'].shape}\")\n",
    "# print(f\"VAE features shape: {multimodal_both['vae_features'].shape}\")\n",
    "# print(f\"Multimodal ready: {multimodal_both['multimodal_ready']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model structure analysis:\")\n",
    "analyze_model_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model components\n",
    "print(\"BAGEL Model Architecture:\")\n",
    "print(f\"Language model: {type(model.language_model).__name__}\")\n",
    "print(f\"Vision model: {type(model.vit_model).__name__}\")\n",
    "print(f\"VAE model: {type(vae_model).__name__}\")\n",
    "print(f\"\\nModel config:\")\n",
    "print(f\"- Visual generation: {config.visual_gen}\")\n",
    "print(f\"- Visual understanding: {config.visual_und}\")\n",
    "print(f\"- Max latent size: {config.max_latent_size}\")\n",
    "print(f\"- Latent patch size: {config.latent_patch_size}\")\n",
    "print(f\"- ViT max patches per side: {config.vit_max_num_patch_per_side}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_representations(representations, filename):\n",
    "    \"\"\"\n",
    "    Save representations to file for further analysis.\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy for saving\n",
    "    numpy_repr = {}\n",
    "    for key, value in representations.items():\n",
    "        if torch.is_tensor(value):\n",
    "            numpy_repr[key] = value.cpu().numpy()\n",
    "        elif isinstance(value, list) and torch.is_tensor(value[0]):\n",
    "            numpy_repr[key] = [v.cpu().numpy() for v in value]\n",
    "        else:\n",
    "            numpy_repr[key] = value\n",
    "    \n",
    "    np.savez(filename, **numpy_repr)\n",
    "    print(f\"Representations saved to {filename}\")\n",
    "\n",
    "# Example: Save text representations\n",
    "save_representations(text_repr, \"text_representations.npz\")\n",
    "\n",
    "# Load back example\n",
    "loaded_repr = np.load(\"text_representations.npz\", allow_pickle=True)\n",
    "print(f\"Loaded representation keys: {list(loaded_repr.keys())}\")\n",
    "print(f\"Loaded embeddings shape: {loaded_repr['embeddings'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAGEL Circuit Breakers\n",
    "\n",
    "This section implements circuit breaker safety mechanisms targeting BAGEL's MoT (Mixture-of-Tokens) architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze MoT Architecture\n",
    "\n",
    "Let's first analyze BAGEL's MoT layers to identify the best intervention points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mot_architecture():\n",
    "    \"\"\"\n",
    "    Analyze BAGEL's MoT (Mixture-of-Tokens) architecture to identify circuit breaker intervention points.\n",
    "    \"\"\"\n",
    "    print(\"BAGEL MoT Architecture Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get the language model\n",
    "    llm = model.language_model.model\n",
    "    print(f\"Language model type: {type(llm).__name__}\")\n",
    "    print(f\"Number of layers: {len(llm.layers)}\")\n",
    "    \n",
    "    # Analyze MoT layers\n",
    "    mot_layers = []\n",
    "    for i, layer in enumerate(llm.layers):\n",
    "        layer_type = type(layer).__name__\n",
    "        print(f\"Layer {i}: {layer_type}\")\n",
    "        \n",
    "        if 'MoT' in layer_type:\n",
    "            mot_layers.append(i)\n",
    "            # Analyze MoT-specific components\n",
    "            print(f\"  ✓ MoT Layer - Components:\")\n",
    "            print(f\"    - self_attn: {type(layer.self_attn).__name__}\")\n",
    "            print(f\"    - mlp: {type(layer.mlp).__name__}\")\n",
    "            \n",
    "            # Check for MoT-specific dual components\n",
    "            if hasattr(layer, 'mlp_moe_gen'):\n",
    "                print(f\"    - mlp_moe_gen: {type(layer.mlp_moe_gen).__name__} (Generation-specific)\")\n",
    "            if hasattr(layer, 'input_layernorm_moe_gen'):\n",
    "                print(f\"    - input_layernorm_moe_gen: {type(layer.input_layernorm_moe_gen).__name__}\")\n",
    "            if hasattr(layer, 'post_attention_layernorm_moe_gen'):\n",
    "                print(f\"    - post_attention_layernorm_moe_gen: {type(layer.post_attention_layernorm_moe_gen).__name__}\")\n",
    "                \n",
    "            # Check for freeze_und attribute\n",
    "            if hasattr(layer, 'freeze_und'):\n",
    "                print(f\"    - freeze_und: {layer.freeze_und}\")\n",
    "    \n",
    "    print(f\"\\nMoT Layers found: {mot_layers}\")\n",
    "    print(f\"Total MoT layers: {len(mot_layers)}\")\n",
    "    \n",
    "    # Recommended intervention points (middle and later layers for safety)\n",
    "    if mot_layers:\n",
    "        num_layers = len(mot_layers)\n",
    "        # Target middle to later layers for circuit breakers\n",
    "        target_layers = []\n",
    "        if num_layers >= 20:\n",
    "            target_layers = [num_layers//2, num_layers*2//3, num_layers*3//4, num_layers-3, num_layers-1]\n",
    "        elif num_layers >= 10:\n",
    "            target_layers = [num_layers//2, num_layers*2//3, num_layers-2, num_layers-1]\n",
    "        else:\n",
    "            target_layers = [num_layers//2, num_layers-1]\n",
    "            \n",
    "        # Ensure target layers are valid indices\n",
    "        target_layers = [l for l in target_layers if l < len(mot_layers)]\n",
    "        \n",
    "        print(f\"\\nRecommended Circuit Breaker Target Layers: {target_layers}\")\n",
    "        print(\"(These target middle-to-late layers for effective safety intervention)\")\n",
    "        \n",
    "        return {\n",
    "            \"total_layers\": len(llm.layers),\n",
    "            \"mot_layers\": mot_layers,\n",
    "            \"target_layers\": target_layers,\n",
    "            \"layer_objects\": [llm.layers[i] for i in target_layers]\n",
    "        }\n",
    "    else:\n",
    "        print(\"\\n⚠️  No MoT layers found!\")\n",
    "        return {\"total_layers\": len(llm.layers), \"mot_layers\": [], \"target_layers\": []}\n",
    "\n",
    "# Run the analysis\n",
    "mot_analysis = analyze_mot_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Breaker Hook System\n",
    "\n",
    "Now let's implement forward hooks to intercept and modify hidden states at the MoT layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Callable, Any\n",
    "import json\n",
    "import random\n",
    "\n",
    "class ImageGenerationCircuitBreakerHooks:\n",
    "    \"\"\"\n",
    "    Manages forward hooks specifically for image generation safety on BAGEL's MoT layers.\n",
    "    Targets the generation-specific components (mlp_moe_gen) to prevent harmful image generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_layers: List[int], layer_objects: List[torch.nn.Module]):\n",
    "        \"\"\"\n",
    "        Initialize hook manager for image generation safety.\n",
    "        \n",
    "        Args:\n",
    "            target_layers: List of layer indices to target\n",
    "            layer_objects: List of actual layer modules to hook\n",
    "        \"\"\"\n",
    "        self.target_layers = target_layers\n",
    "        self.layer_objects = layer_objects\n",
    "        self.hooks = []\n",
    "        self.generation_interventions = {}\n",
    "        self.collected_activations = {}\n",
    "        self.safety_enabled = False\n",
    "        \n",
    "        # Image generation safety parameters\n",
    "        self.steering_strength = 0.2  # Stronger intervention for generation\n",
    "        self.intervention_mode = \"generation_blocking\"  # Specific to image generation\n",
    "        self.generation_threshold = 0.5  # Threshold for harmful content detection\n",
    "        \n",
    "    def register_generation_hooks(self):\n",
    "        \"\"\"Register forward hooks specifically targeting generation components.\"\"\"\n",
    "        self.hooks = []\n",
    "        \n",
    "        for i, (layer_idx, layer_obj) in enumerate(zip(self.target_layers, self.layer_objects)):\n",
    "            # Check if this layer has generation-specific components\n",
    "            if hasattr(layer_obj, 'mlp_moe_gen'):\n",
    "                print(f\"Found generation-specific MLP in layer {layer_idx}\")\n",
    "                \n",
    "                # Hook the generation-specific MLP\n",
    "                def make_gen_hook(layer_index):\n",
    "                    def generation_hook(module, input, output):\n",
    "                        return self._generation_intervention_hook(layer_index, module, input, output)\n",
    "                    return generation_hook\n",
    "                \n",
    "                gen_hook = layer_obj.mlp_moe_gen.register_forward_hook(make_gen_hook(layer_idx))\n",
    "                self.hooks.append(gen_hook)\n",
    "                \n",
    "                # Also hook the generation-specific layer norms if they exist\n",
    "                if hasattr(layer_obj, 'input_layernorm_moe_gen'):\n",
    "                    norm_hook = layer_obj.input_layernorm_moe_gen.register_forward_hook(make_gen_hook(layer_idx))\n",
    "                    self.hooks.append(norm_hook)\n",
    "                \n",
    "                print(f\"Registered generation safety hooks on layer {layer_idx}\")\n",
    "            else:\n",
    "                print(f\"Layer {layer_idx} does not have generation-specific components\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        print(\"All generation safety hooks removed\")\n",
    "    \n",
    "    def _generation_intervention_hook(self, layer_idx: int, module, input, output):\n",
    "        \"\"\"\n",
    "        Intervention hook specifically for image generation components.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: Index of the layer\n",
    "            module: The generation module (e.g., mlp_moe_gen)\n",
    "            input: Input to the module\n",
    "            output: Output from the module\n",
    "            \n",
    "        Returns:\n",
    "            Modified output if safety is enabled and harmful generation detected\n",
    "        \"\"\"\n",
    "        # Always collect activations for analysis\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            self.collected_activations[f\"layer_{layer_idx}_gen\"] = output.detach().clone()\n",
    "        \n",
    "        # Apply safety intervention if enabled\n",
    "        if self.safety_enabled and layer_idx in self.generation_interventions:\n",
    "            return self._apply_generation_safety(layer_idx, module, input, output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _apply_generation_safety(self, layer_idx: int, module, input, output):\n",
    "        \"\"\"\n",
    "        Apply safety intervention specifically for image generation.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: Layer index\n",
    "            module: Generation module\n",
    "            input: Module input\n",
    "            output: Module output\n",
    "            \n",
    "        Returns:\n",
    "            Modified output that blocks harmful generation\n",
    "        \"\"\"\n",
    "        intervention = self.generation_interventions[layer_idx]\n",
    "        \n",
    "        if self.intervention_mode == \"generation_blocking\":\n",
    "            # Detect if this is likely harmful generation\n",
    "            if self._detect_harmful_generation_pattern(output):\n",
    "                print(f\"🛑 Circuit breaker activated at layer {layer_idx} - blocking harmful generation\")\n",
    "                \n",
    "                # Replace with safe generation pattern\n",
    "                safe_output = self._get_safe_generation_output(output, intervention)\n",
    "                return safe_output\n",
    "        \n",
    "        elif self.intervention_mode == \"steering\":\n",
    "            # Apply steering away from harmful generation\n",
    "            steering_vector = intervention.get(\"safety_steering\", torch.zeros_like(output))\n",
    "            return output + self.steering_strength * steering_vector\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _detect_harmful_generation_pattern(self, output: torch.Tensor) -> bool:\n",
    "        \"\"\"\n",
    "        Simple harmful generation pattern detection.\n",
    "        In practice, this could use learned classifiers or more sophisticated methods.\n",
    "        \n",
    "        Args:\n",
    "            output: Tensor output from generation component\n",
    "            \n",
    "        Returns:\n",
    "            True if harmful pattern detected\n",
    "        \"\"\"\n",
    "        # Simple heuristic: look for high activation patterns that might indicate harmful content\n",
    "        # This is a placeholder - in practice you'd use learned detection\n",
    "        \n",
    "        # Check for unusually high activations (potential sign of harmful generation)\n",
    "        max_activation = output.abs().max().item()\n",
    "        mean_activation = output.abs().mean().item()\n",
    "        \n",
    "        # Simple threshold-based detection\n",
    "        if max_activation > 3.0 * mean_activation and max_activation > self.generation_threshold:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _get_safe_generation_output(self, original_output: torch.Tensor, intervention: Dict[str, Any]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Replace harmful generation output with safe alternative.\n",
    "        \n",
    "        Args:\n",
    "            original_output: Original potentially harmful output\n",
    "            intervention: Intervention parameters\n",
    "            \n",
    "        Returns:\n",
    "            Safe generation output\n",
    "        \"\"\"\n",
    "        # Option 1: Use a learned safe replacement\n",
    "        if \"safe_replacement\" in intervention:\n",
    "            return intervention[\"safe_replacement\"]\n",
    "        \n",
    "        # Option 2: Zero out high activations (conservative approach)\n",
    "        safe_output = original_output.clone()\n",
    "        threshold = self.generation_threshold\n",
    "        \n",
    "        # Clamp extreme values that might lead to harmful generation\n",
    "        safe_output = torch.clamp(safe_output, -threshold, threshold)\n",
    "        \n",
    "        # Add some noise to prevent memorization of the clamping pattern\n",
    "        noise = torch.randn_like(safe_output) * 0.01\n",
    "        safe_output = safe_output + noise\n",
    "        \n",
    "        return safe_output\n",
    "    \n",
    "    def set_generation_intervention(self, layer_idx: int, intervention_type: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Set intervention parameters for generation safety.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: Layer index\n",
    "            intervention_type: Type of intervention (\"blocking\", \"steering\", etc.)\n",
    "            **kwargs: Intervention parameters\n",
    "        \"\"\"\n",
    "        self.generation_interventions[layer_idx] = {\n",
    "            \"type\": intervention_type,\n",
    "            **kwargs\n",
    "        }\n",
    "        print(f\"Set {intervention_type} generation safety intervention for layer {layer_idx}\")\n",
    "    \n",
    "    def enable_generation_safety(self):\n",
    "        \"\"\"Enable generation safety interventions.\"\"\"\n",
    "        self.safety_enabled = True\n",
    "        print(\"🛡️ Image generation safety enabled\")\n",
    "    \n",
    "    def disable_generation_safety(self):\n",
    "        \"\"\"Disable generation safety interventions.\"\"\"\n",
    "        self.safety_enabled = False\n",
    "        print(\"Image generation safety disabled\")\n",
    "    \n",
    "    def get_generation_activations(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Get collected activations from generation components.\"\"\"\n",
    "        return self.collected_activations.copy()\n",
    "    \n",
    "    def clear_activations(self):\n",
    "        \"\"\"Clear collected activations.\"\"\"\n",
    "        self.collected_activations = {}\n",
    "    \n",
    "    def analyze_generation_pathway(self):\n",
    "        \"\"\"Analyze which layers have generation-specific components.\"\"\"\n",
    "        print(\"Analyzing BAGEL's Generation Pathway:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        generation_layers = []\n",
    "        for layer_idx, layer_obj in zip(self.target_layers, self.layer_objects):\n",
    "            has_gen_components = False\n",
    "            components = []\n",
    "            \n",
    "            if hasattr(layer_obj, 'mlp_moe_gen'):\n",
    "                components.append(\"mlp_moe_gen\")\n",
    "                has_gen_components = True\n",
    "            \n",
    "            if hasattr(layer_obj, 'input_layernorm_moe_gen'):\n",
    "                components.append(\"input_layernorm_moe_gen\")\n",
    "                has_gen_components = True\n",
    "            \n",
    "            if hasattr(layer_obj, 'post_attention_layernorm_moe_gen'):\n",
    "                components.append(\"post_attention_layernorm_moe_gen\")\n",
    "                has_gen_components = True\n",
    "            \n",
    "            if has_gen_components:\n",
    "                generation_layers.append(layer_idx)\n",
    "                print(f\"Layer {layer_idx}: {', '.join(components)}\")\n",
    "            else:\n",
    "                print(f\"Layer {layer_idx}: No generation-specific components\")\n",
    "        \n",
    "        print(f\"\\nGeneration-capable layers: {generation_layers}\")\n",
    "        print(f\"Total generation layers: {len(generation_layers)}\")\n",
    "        \n",
    "        return generation_layers\n",
    "\n",
    "# Initialize image generation circuit breaker hooks\n",
    "if mot_analysis[\"target_layers\"]:\n",
    "    img_cb_hooks = ImageGenerationCircuitBreakerHooks(\n",
    "        target_layers=mot_analysis[\"target_layers\"],\n",
    "        layer_objects=mot_analysis[\"layer_objects\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Image Generation Circuit Breaker Hooks initialized!\")\n",
    "    \n",
    "    # Analyze the generation pathway\n",
    "    generation_layers = img_cb_hooks.analyze_generation_pathway()\n",
    "    \n",
    "else:\n",
    "    img_cb_hooks = None\n",
    "    print(\"⚠️  No MoT layers found - image generation circuit breaker hooks not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Breaker Dataset\n",
    "\n",
    "Adapt the proven circuit breaker training approach for BAGEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I2PImageGenerationDataset:\n",
    "    \"\"\"\n",
    "    Proper I2P dataset integration for image generation safety.\n",
    "    Uses the real I2P annotations: categories, inappropriate_percentage, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length=512, \n",
    "                 harmful_threshold=70.0, safe_threshold=30.0, \n",
    "                 nudity_threshold=20.0, hard_only=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.harmful_threshold = harmful_threshold\n",
    "        self.safe_threshold = safe_threshold\n",
    "        self.nudity_threshold = nudity_threshold\n",
    "        self.hard_only = hard_only\n",
    "        \n",
    "        # Load I2P dataset\n",
    "        print(\"Loading I2P dataset...\")\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            self.i2p_dataset = load_dataset(\"AIML-TUDA/i2p\")\n",
    "            print(f\"✓ I2P dataset loaded: {len(self.i2p_dataset['train'])} total prompts\")\n",
    "            \n",
    "            # Process the dataset\n",
    "            self._process_i2p_dataset()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to load I2P dataset: {e}\")\n",
    "            raise RuntimeError(\"I2P dataset is required for this implementation\")\n",
    "    \n",
    "    def _process_i2p_dataset(self):\n",
    "        \"\"\"Process I2P dataset based on safety annotations.\"\"\"\n",
    "        \n",
    "        self.safe_examples = []\n",
    "        self.harmful_examples = []\n",
    "        self.category_stats = {}\n",
    "        \n",
    "        print(\"Processing I2P dataset annotations...\")\n",
    "        \n",
    "        for item in self.i2p_dataset['train']:\n",
    "            # Extract fields from I2P format\n",
    "            prompt = item['prompt']\n",
    "            categories = item.get('categories', 'unknown')\n",
    "            hard = item.get('hard', 0)\n",
    "            inappropriate_pct = item.get('inappropriate_percentage', 0)\n",
    "            nudity_pct = item.get('nudity_percentage', 0)\n",
    "            sd_safety_pct = item.get('sd_safety_percentage', 100)\n",
    "            toxicity = item.get('prompt_toxicity', 0)\n",
    "            \n",
    "            # Skip if hard_only is enabled and this isn't hard\n",
    "            if self.hard_only and hard == 0:\n",
    "                continue\n",
    "            \n",
    "            # Count categories\n",
    "            if categories not in self.category_stats:\n",
    "                self.category_stats[categories] = {\"total\": 0, \"harmful\": 0, \"safe\": 0}\n",
    "            self.category_stats[categories][\"total\"] += 1\n",
    "            \n",
    "            # Determine if harmful based on multiple criteria\n",
    "            is_harmful = (\n",
    "                inappropriate_pct >= self.harmful_threshold or \n",
    "                nudity_pct >= self.nudity_threshold or\n",
    "                sd_safety_pct <= 30  # Low SD safety score indicates harmful\n",
    "            )\n",
    "            \n",
    "            # Determine if safe based on criteria\n",
    "            is_safe = (\n",
    "                inappropriate_pct <= self.safe_threshold and \n",
    "                nudity_pct <= 10 and\n",
    "                sd_safety_pct >= 80 and\n",
    "                toxicity <= 0.1\n",
    "            )\n",
    "            \n",
    "            # Create example data structure\n",
    "            example = {\n",
    "                \"prompt\": prompt,\n",
    "                \"categories\": categories,\n",
    "                \"hard\": hard,\n",
    "                \"inappropriate_percentage\": inappropriate_pct,\n",
    "                \"nudity_percentage\": nudity_pct,\n",
    "                \"sd_safety_percentage\": sd_safety_pct,\n",
    "                \"prompt_toxicity\": toxicity,\n",
    "                \"full_text\": f\"Generate an image: {prompt}\",\n",
    "            }\n",
    "            \n",
    "            if is_harmful:\n",
    "                example.update({\n",
    "                    \"type\": \"harmful_generation\",\n",
    "                    \"should_generate\": False\n",
    "                })\n",
    "                self.harmful_examples.append(example)\n",
    "                self.category_stats[categories][\"harmful\"] += 1\n",
    "                \n",
    "            elif is_safe:\n",
    "                example.update({\n",
    "                    \"type\": \"safe_generation\", \n",
    "                    \"should_generate\": True\n",
    "                })\n",
    "                self.safe_examples.append(example)\n",
    "                self.category_stats[categories][\"safe\"] += 1\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\nI2P Dataset Processing Results:\")\n",
    "        print(f\"- Safe examples: {len(self.safe_examples)}\")\n",
    "        print(f\"- Harmful examples: {len(self.harmful_examples)}\")\n",
    "        print(f\"- Hard examples only: {self.hard_only}\")\n",
    "        print(f\"- Harmful threshold: {self.harmful_threshold}%\")\n",
    "        print(f\"- Safe threshold: {self.safe_threshold}%\")\n",
    "        \n",
    "        print(f\"\\nCategory Breakdown:\")\n",
    "        for category, stats in self.category_stats.items():\n",
    "            print(f\"  {category}: {stats['total']} total, {stats['harmful']} harmful, {stats['safe']} safe\")\n",
    "    \n",
    "    def get_examples_by_category(self, category: str, harmful: bool = True):\n",
    "        \"\"\"Get examples filtered by category.\"\"\"\n",
    "        examples = self.harmful_examples if harmful else self.safe_examples\n",
    "        return [ex for ex in examples if ex[\"categories\"] == category]\n",
    "    \n",
    "    def get_high_risk_examples(self, min_inappropriate=90, min_nudity=50):\n",
    "        \"\"\"Get the most problematic examples for robust training.\"\"\"\n",
    "        high_risk = []\n",
    "        for ex in self.harmful_examples:\n",
    "            if (ex[\"inappropriate_percentage\"] >= min_inappropriate or \n",
    "                ex[\"nudity_percentage\"] >= min_nudity):\n",
    "                high_risk.append(ex)\n",
    "        return high_risk\n",
    "    \n",
    "    def tokenize_for_generation(self, examples):\n",
    "        \"\"\"Tokenize examples for image generation training.\"\"\"\n",
    "        tokenized = []\n",
    "        \n",
    "        for example in examples:\n",
    "            # Tokenize the prompt\n",
    "            tokens = self.tokenizer(\n",
    "                example[\"prompt\"],\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            tokenized.append({\n",
    "                \"input_ids\": tokens[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": tokens[\"attention_mask\"].squeeze(),\n",
    "                \"prompt\": example[\"prompt\"],\n",
    "                \"type\": example[\"type\"],\n",
    "                \"should_generate\": example[\"should_generate\"],\n",
    "                \"categories\": example[\"categories\"],\n",
    "                \"inappropriate_percentage\": example[\"inappropriate_percentage\"],\n",
    "                \"nudity_percentage\": example[\"nudity_percentage\"],\n",
    "                \"sd_safety_percentage\": example[\"sd_safety_percentage\"],\n",
    "                \"prompt_toxicity\": example[\"prompt_toxicity\"],\n",
    "                \"hard\": example[\"hard\"]\n",
    "            })\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    def get_training_batch(self, batch_size=8, safe_ratio=0.5, \n",
    "                          target_categories=None, min_difficulty=None):\n",
    "        \"\"\"\n",
    "        Get a training batch with sophisticated filtering.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of examples in batch\n",
    "            safe_ratio: Ratio of safe to harmful examples\n",
    "            target_categories: List of categories to focus on (e.g., ['sexual', 'violence'])\n",
    "            min_difficulty: Minimum inappropriate_percentage for harmful examples\n",
    "        \"\"\"\n",
    "        n_safe = int(batch_size * safe_ratio)\n",
    "        n_harmful = batch_size - n_safe\n",
    "        \n",
    "        # Filter examples based on criteria\n",
    "        available_safe = self.safe_examples.copy()\n",
    "        available_harmful = self.harmful_examples.copy()\n",
    "        \n",
    "        # Filter by categories if specified\n",
    "        if target_categories:\n",
    "            available_safe = [ex for ex in available_safe if ex[\"categories\"] in target_categories]\n",
    "            available_harmful = [ex for ex in available_harmful if ex[\"categories\"] in target_categories]\n",
    "        \n",
    "        # Filter by minimum difficulty\n",
    "        if min_difficulty:\n",
    "            available_harmful = [ex for ex in available_harmful \n",
    "                               if ex[\"inappropriate_percentage\"] >= min_difficulty]\n",
    "        \n",
    "        # Sample examples\n",
    "        safe_batch = random.sample(\n",
    "            available_safe, \n",
    "            min(n_safe, len(available_safe))\n",
    "        )\n",
    "        harmful_batch = random.sample(\n",
    "            available_harmful, \n",
    "            min(n_harmful, len(available_harmful))\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        safe_tokenized = self.tokenize_for_generation(safe_batch)\n",
    "        harmful_tokenized = self.tokenize_for_generation(harmful_batch)\n",
    "        \n",
    "        return {\n",
    "            \"safe_generation\": safe_tokenized,\n",
    "            \"harmful_generation\": harmful_tokenized,\n",
    "            \"safe_ratio\": safe_ratio,\n",
    "            \"batch_categories\": target_categories,\n",
    "            \"min_difficulty\": min_difficulty\n",
    "        }\n",
    "    \n",
    "    def get_test_prompts(self, n_safe=5, n_harmful=5, categories=None):\n",
    "        \"\"\"Get test prompts with category filtering.\"\"\"\n",
    "        available_safe = self.safe_examples\n",
    "        available_harmful = self.harmful_examples\n",
    "        \n",
    "        if categories:\n",
    "            available_safe = [ex for ex in available_safe if ex[\"categories\"] in categories]\n",
    "            available_harmful = [ex for ex in available_harmful if ex[\"categories\"] in categories]\n",
    "        \n",
    "        safe_test = random.sample(available_safe, min(n_safe, len(available_safe)))\n",
    "        harmful_test = random.sample(available_harmful, min(n_harmful, len(available_harmful)))\n",
    "        \n",
    "        return {\n",
    "            \"safe_prompts\": [{\"prompt\": ex[\"prompt\"], \"metadata\": ex} for ex in safe_test],\n",
    "            \"harmful_prompts\": [{\"prompt\": ex[\"prompt\"], \"metadata\": ex} for ex in harmful_test]\n",
    "        }\n",
    "    \n",
    "    def analyze_dataset(self):\n",
    "        \"\"\"Analyze the I2P dataset characteristics.\"\"\"\n",
    "        print(\"I2P Dataset Analysis:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_examples = len(self.safe_examples) + len(self.harmful_examples)\n",
    "        print(f\"Total usable examples: {total_examples}\")\n",
    "        print(f\"Safe examples: {len(self.safe_examples)} ({len(self.safe_examples)/total_examples*100:.1f}%)\")\n",
    "        print(f\"Harmful examples: {len(self.harmful_examples)} ({len(self.harmful_examples)/total_examples*100:.1f}%)\")\n",
    "        \n",
    "        # Difficulty distribution\n",
    "        if self.harmful_examples:\n",
    "            inappropriateness_scores = [ex[\"inappropriate_percentage\"] for ex in self.harmful_examples]\n",
    "            print(f\"\\nInappropriateness Distribution (Harmful Examples):\")\n",
    "            print(f\"  Mean: {np.mean(inappropriateness_scores):.1f}%\")\n",
    "            print(f\"  Median: {np.median(inappropriateness_scores):.1f}%\")\n",
    "            print(f\"  Min: {min(inappropriateness_scores):.1f}%\")\n",
    "            print(f\"  Max: {max(inappropriateness_scores):.1f}%\")\n",
    "        \n",
    "        # Category analysis\n",
    "        print(f\"\\nCategory Distribution:\")\n",
    "        sorted_categories = sorted(self.category_stats.items(), \n",
    "                                 key=lambda x: x[1][\"total\"], reverse=True)\n",
    "        for category, stats in sorted_categories[:10]:  # Top 10 categories\n",
    "            total = stats[\"total\"]\n",
    "            harmful_pct = stats[\"harmful\"] / total * 100 if total > 0 else 0\n",
    "            print(f\"  {category}: {total} total ({harmful_pct:.1f}% harmful)\")\n",
    "\n",
    "# Create proper I2P-based dataset\n",
    "print(\"Creating I2P-based Image Generation Safety Dataset...\")\n",
    "i2p_dataset = I2PImageGenerationDataset(\n",
    "    tokenizer, \n",
    "    max_length=512, \n",
    "    harmful_threshold=70.0,  # 70%+ inappropriate content = harmful\n",
    "    safe_threshold=30.0,     # <30% inappropriate content = safe\n",
    "    nudity_threshold=20.0,   # 20%+ nudity = harmful\n",
    "    hard_only=False          # Use both easy and hard examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Generation Safety Testing\n",
    "\n",
    "Test the circuit breaker system with safe and harmful image generation prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_i2p_image_generation_safety():\n",
    "    \"\"\"\n",
    "    Test the image generation circuit breaker system with real I2P data.\n",
    "    \"\"\"\n",
    "    if img_cb_hooks is None:\n",
    "        print(\"⚠️ Image generation circuit breakers not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"Testing BAGEL Image Generation Safety with I2P Dataset\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Analyze the I2P dataset first\n",
    "    i2p_dataset.analyze_dataset()\n",
    "    \n",
    "    # Get test prompts from real I2P data\n",
    "    test_prompts = i2p_dataset.get_test_prompts(n_safe=3, n_harmful=3)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"🟢 Safe Prompts from I2P (low inappropriate %):\")\n",
    "    for i, item in enumerate(test_prompts[\"safe_prompts\"]):\n",
    "        prompt = item[\"prompt\"]\n",
    "        meta = item[\"metadata\"]\n",
    "        print(f\"  {i+1}. {prompt[:80]}...\")\n",
    "        print(f\"     Category: {meta['categories']}, Inappropriate: {meta['inappropriate_percentage']}%\")\n",
    "    \n",
    "    print(f\"\\n🔴 Harmful Prompts from I2P (high inappropriate %):\")\n",
    "    for i, item in enumerate(test_prompts[\"harmful_prompts\"]):\n",
    "        prompt = item[\"prompt\"]\n",
    "        meta = item[\"metadata\"]\n",
    "        print(f\"  {i+1}. {prompt[:80]}...\")\n",
    "        print(f\"     Category: {meta['categories']}, Inappropriate: {meta['inappropriate_percentage']}%\")\n",
    "        print(f\"     Nudity: {meta['nudity_percentage']}%, Toxicity: {meta['prompt_toxicity']:.3f}\")\n",
    "    \n",
    "    # Test category-specific examples\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Category-Specific Testing:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test different categories\n",
    "    test_categories = ['sexual', 'violence', 'hate', 'shocking']\n",
    "    for category in test_categories:\n",
    "        category_examples = i2p_dataset.get_examples_by_category(category, harmful=True)\n",
    "        if category_examples:\n",
    "            example = category_examples[0]\n",
    "            print(f\"\\n🎯 {category.upper()} category example:\")\n",
    "            print(f\"   Prompt: {example['prompt'][:100]}...\")\n",
    "            print(f\"   Inappropriate: {example['inappropriate_percentage']}%\")\n",
    "            print(f\"   SD Safety: {example['sd_safety_percentage']}%\")\n",
    "    \n",
    "    # Test high-risk examples\n",
    "    high_risk = i2p_dataset.get_high_risk_examples(min_inappropriate=95, min_nudity=80)\n",
    "    print(f\"\\n⚠️ High-Risk Examples (95%+ inappropriate OR 80%+ nudity): {len(high_risk)} found\")\n",
    "    if high_risk:\n",
    "        example = high_risk[0]\n",
    "        print(f\"   Most problematic: {example['prompt'][:100]}...\")\n",
    "        print(f\"   Inappropriate: {example['inappropriate_percentage']}%, Nudity: {example['nudity_percentage']}%\")\n",
    "    \n",
    "    # Test circuit breaker training batch generation\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Training Batch Generation:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test different training configurations\n",
    "    configs = [\n",
    "        {\"safe_ratio\": 0.5, \"target_categories\": None, \"min_difficulty\": None},\n",
    "        {\"safe_ratio\": 0.3, \"target_categories\": ['sexual', 'violence'], \"min_difficulty\": 80},\n",
    "        {\"safe_ratio\": 0.7, \"target_categories\": ['hate'], \"min_difficulty\": 70}\n",
    "    ]\n",
    "    \n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"\\nTraining Config {i+1}: {config}\")\n",
    "        try:\n",
    "            batch = i2p_dataset.get_training_batch(batch_size=6, **config)\n",
    "            print(f\"  ✓ Generated batch: {len(batch['safe_generation'])} safe, {len(batch['harmful_generation'])} harmful\")\n",
    "            \n",
    "            # Show sample from batch\n",
    "            if batch['harmful_generation']:\n",
    "                sample = batch['harmful_generation'][0]\n",
    "                print(f\"    Sample harmful: {sample['prompt'][:60]}... ({sample['categories']}, {sample['inappropriate_percentage']}%)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Batch generation failed: {e}\")\n",
    "    \n",
    "    # Test safety system integration\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Circuit Breaker Integration Test:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Enable circuit breaker hooks\n",
    "    try:\n",
    "        img_cb_hooks.register_generation_hooks()\n",
    "        img_cb_hooks.enable_generation_safety()\n",
    "        \n",
    "        # Test with a real harmful prompt from I2P\n",
    "        if test_prompts[\"harmful_prompts\"]:\n",
    "            harmful_item = test_prompts[\"harmful_prompts\"][0]\n",
    "            harmful_prompt = harmful_item[\"prompt\"]\n",
    "            harmful_meta = harmful_item[\"metadata\"]\n",
    "            \n",
    "            print(f\"\\\\nTesting circuit breakers with real harmful prompt:\")\n",
    "            print(f\"Prompt: {harmful_prompt[:100]}...\")\n",
    "            print(f\"I2P Rating: {harmful_meta['inappropriate_percentage']}% inappropriate\")\n",
    "            \n",
    "            # Tokenize and test\n",
    "            tokens = tokenizer(harmful_prompt, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "            input_ids = tokens[\"input_ids\"].to(next(model.parameters()).device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                img_cb_hooks.clear_activations()\n",
    "                # This would normally go through full generation pipeline\n",
    "                embeddings = model.language_model.model.embed_tokens(input_ids)\n",
    "                activations = img_cb_hooks.get_generation_activations()\n",
    "                \n",
    "                print(f\"✓ Processed with safety monitoring\")\n",
    "                print(f\"  Activations captured: {len(activations)} generation layers\")\n",
    "                print(f\"  Ready for circuit breaker training with I2P annotations\")\n",
    "        \n",
    "        # Clean up\n",
    "        img_cb_hooks.remove_hooks()\n",
    "        img_cb_hooks.disable_generation_safety()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Circuit breaker integration error: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"I2P Integration Summary:\")\n",
    "    print(\"✓ Real inappropriate image prompts loaded from I2P dataset\")\n",
    "    print(\"✓ Multi-dimensional safety ratings (inappropriate %, nudity %, toxicity)\")\n",
    "    print(\"✓ Category-specific training capability\") \n",
    "    print(\"✓ Difficulty-based filtering for robust training\")\n",
    "    print(\"✓ Integration with MoT generation circuit breakers\")\n",
    "    print(\"✓ Ready for progressive training with retain vs circuit breaker loss\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Run the I2P-based safety test\n",
    "test_i2p_image_generation_safety()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
